import json
from openai import OpenAI
from typing import Dict, Any, Optional, Tuple, List, Union
import os
from dotenv import load_dotenv
import logging

logger = logging.getLogger(__name__)

def get_openai_api_key():
    """Biztons√°gos API kulcs lek√©r√©s Streamlit f√ºgg≈ës√©gek n√©lk√ºl"""
    load_dotenv()
    return os.getenv("OPENAI_API_KEY")

class AISummaryGenerator:
    """
    OpenAI API-val t√∂rt√©n≈ë √∂sszefoglal√≥ √©s javaslat gener√°l√°s - OPTIMALIZ√ÅLT VERZI√ì
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """
        Inicializ√°lja az AI Summary Generator-t
        
        Args:
            api_key: OpenAI API kulcs (ha nincs megadva, k√∂rnyezetb≈ël veszi)
        """
        self.api_key = api_key or get_openai_api_key()
        if not self.api_key:
            raise ValueError("OpenAI API kulcs sz√ºks√©ges. √Åll√≠tsd be a OPENAI_API_KEY environment v√°ltoz√≥t.")
        
        self.client = OpenAI(api_key=self.api_key)
    
    def generate_summary_and_recommendations(self, json_data: Union[Dict[str, Any], List[Dict[str, Any]]]) -> Tuple[str, str]:
        """
        Gener√°l egy √∂sszefoglal√≥t √©s javaslatokat a JSON adatok alapj√°n
        
        Args:
            json_data: Az elemz√©s eredm√©ny√©t tartalmaz√≥ JSON adatok (dict vagy list)
            
        Returns:
            Tuple[str, str]: (√∂sszefoglal√≥, javaslatok)
        """
        try:
            # Kompakt, de teljes k√∂r≈± adatkinyer√©s
            analysis_summary = self._create_compact_summary(json_data)
            
            # Token m√©ret ellen≈ërz√©s √©s optimaliz√°l√°s
            json_str = json.dumps(analysis_summary, indent=2, ensure_ascii=False)
            estimated_tokens = len(json_str) // 4  # Durva becsl√©s
            
            # Ha t√∫l nagy, tov√°bbi t√∂m√∂r√≠t√©s
            if estimated_tokens > 4000:
                analysis_summary = self._ultra_compact_summary(analysis_summary)
                json_str = json.dumps(analysis_summary, indent=2, ensure_ascii=False)
            
            # OpenAI API h√≠v√°s - ER≈êS JSON K√âNYSZER√çT√âS
            try:
                # El≈ësz√∂r pr√≥b√°ljuk response_format param√©terrel (GPT-4-turbo t√°mogatja)
                response = self.client.chat.completions.create(
                    model="gpt-4-turbo-preview",  # vagy "gpt-4-1106-preview"
                    response_format={"type": "json_object"},  # JSON m√≥d
                    messages=[
                        {
                            "role": "system",
                            "content": """Te egy GEO (Generative Engine Optimization) szak√©rt≈ë vagy.
V√°laszolj CSAK valid JSON form√°tumban a megadott strukt√∫r√°ban."""
                        },
                        {
                            "role": "user", 
                            "content": f"""Elemezd ezt a GEO audit eredm√©nyt √©s k√©sz√≠ts r√©szletes √∂sszefoglal√≥t √©s javaslatokat.

AUDIT EREDM√âNYEK:
{json_str}

K√©sz√≠ts:
1. R√©szletes √ñSSZEFOGLAL√ì (600-800 sz√≥), amely tartalmazza:
   - AI Readiness Score √©rt√©kel√©se (√°tlag: {analysis_summary.get('overview', {}).get('avg_ai_score', 0)}/100)
   - URL-enk√©nti teljes√≠tm√©ny
   - F≈ëbb probl√©m√°k gyakoris√°ga
   - Platform kompatibilit√°s
   - Technikai hi√°nyoss√°gok

2. Konkr√©t JAVASLATOK (600-800 sz√≥), prioritiz√°lva:
   - Kritikus jav√≠t√°sok
   - Quick wins (gyors eredm√©nyek)
   - Platform-specifikus optimaliz√°ci√≥k
   - V√°rhat√≥ score javul√°s

V√°laszolj PONTOSAN ebben a JSON strukt√∫r√°ban:
{{
    "summary": "Ide √≠rd a r√©szletes √∂sszefoglal√≥t. Haszn√°lj konkr√©t sz√°mokat, sz√°zal√©kokat. Eml√≠tsd meg az √°tlagos AI score-t, a legjobb √©s legrosszabb teljes√≠tm√©ny≈± URL-eket, a leggyakoribb probl√©m√°kat.",
    "recommendations": "Ide √≠rd a prioritiz√°lt javaslatokat. Kezdd a kritikus probl√©m√°kkal, majd a quick wins lehet≈ës√©gekkel. Adj konkr√©t megold√°sokat √©s becs√ºld meg a v√°rhat√≥ javul√°st."
}}"""
                        }
                    ],
                    temperature=0.7,
                    max_tokens=2500
                )
            except Exception as e:
                # Ha nem t√°mogatja a response_format-ot, haszn√°ljuk a standard m√≥dot
                logger.info(f"JSON response format nem t√°mogatott, standard m√≥d: {e}")
                response = self.client.chat.completions.create(
                    model="gpt-4",
                    messages=[
                        {
                            "role": "system",
                            "content": """Te egy GEO szak√©rt≈ë vagy. 
KRITIKUS: V√°laszod CSAK valid JSON lehet {"summary": "...", "recommendations": "..."} form√°tumban!
Ne haszn√°lj markdown-t, csak tiszta JSON-t!"""
                        },
                        {
                            "role": "user", 
                            "content": f"""GEO audit elemz√©se:

{json_str}

√Åtlag AI Score: {analysis_summary.get('overview', {}).get('avg_ai_score', 0)}/100
URL-ek sz√°ma: {analysis_summary.get('overview', {}).get('urls_analyzed', 0)}

Top probl√©m√°k: {', '.join([f"{issue[0]} ({issue[1]}x)" for issue in analysis_summary.get('common_issues', [])[:3]])}

FONTOS: V√°laszolj CSAK ezzel a JSON strukt√∫r√°val:
{{"summary": "r√©szletes √∂sszefoglal√≥ sz√∂veg", "recommendations": "konkr√©t javaslatok sz√∂vege"}}"""
                        }
                    ],
                    temperature=0.7,
                    max_tokens=2500
                )
            
            # V√°lasz feldolgoz√°sa - ROBUSZTUSABB JSON PARSING
            ai_response = response.choices[0].message.content.strip()
            logger.info(f"OpenAI v√°lasz els≈ë 200 karakter: {ai_response[:200]}...")
            
            # Tiszt√≠tsuk meg a v√°laszt minden felesleges karaktert≈ël
            cleaned_response = ai_response
            
            # Markdown code block elt√°vol√≠t√°sa
            if "```json" in cleaned_response:
                cleaned_response = cleaned_response.split("```json")[-1].split("```")[0].strip()
            elif "```" in cleaned_response:
                parts = cleaned_response.split("```")
                if len(parts) >= 2:
                    cleaned_response = parts[1].strip()
            
            # Tov√°bbi tiszt√≠t√°s
            cleaned_response = cleaned_response.strip()
            if cleaned_response.startswith("json"):
                cleaned_response = cleaned_response[4:].strip()
            
            logger.info(f"Tiszt√≠tott v√°lasz els≈ë 200 karakter: {cleaned_response[:200]}...")
            
            # Els≈ë pr√≥b√°lkoz√°s: tiszta JSON parse
            try:
                parsed_response = json.loads(cleaned_response)
                summary = parsed_response.get("summary", "")
                recommendations = parsed_response.get("recommendations", "")
                
                # Ellen≈ërizz√ºk, hogy val√≥di tartalom-e
                if summary and recommendations and len(summary) > 100 and len(recommendations) > 100:
                    logger.info("JSON parsing sikeres")
                    return summary, recommendations
                else:
                    logger.warning("JSON parse sikeres de t√∫l r√∂vid vagy √ºres a tartalom")
                    raise ValueError("Incomplete response")
                    
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"JSON parse hiba: {e}")
                logger.warning(f"Probl√©m√°s JSON: {cleaned_response[:500]}...")
                
                # M√°sodik pr√≥b√°lkoz√°s: regex alap√∫ kinyer√©s
                import re
                
                # T√∂bbf√©le pattern pr√≥b√°lkoz√°s
                patterns = [
                    # Standard JSON
                    (r'"summary"\s*:\s*"([^"]*(?:\\.[^"]*)*)"', r'"recommendations"\s*:\s*"([^"]*(?:\\.[^"]*)*)"'),
                    # Aposztr√≥f
                    (r"'summary'\s*:\s*'([^']*(?:\\.[^']*)*)'", r"'recommendations'\s*:\s*'([^']*(?:\\.[^']*)*)'"),
                    # T√∂bbsoros JSON
                    (r'"summary"\s*:\s*"([\s\S]*?)"(?:,\s*"recommendations")', r'"recommendations"\s*:\s*"([\s\S]*?)"(?:\s*\})'),
                ]
                
                summary = None
                recommendations = None
                
                for sum_pattern, rec_pattern in patterns:
                    if not summary:
                        sum_match = re.search(sum_pattern, cleaned_response, re.DOTALL)
                        if sum_match:
                            summary = sum_match.group(1)
                            # Escape karakterek kezel√©se
                            summary = summary.replace('\\n', '\n').replace('\\"', '"').replace('\\t', '    ')
                    
                    if not recommendations:
                        rec_match = re.search(rec_pattern, cleaned_response, re.DOTALL)
                        if rec_match:
                            recommendations = rec_match.group(1)
                            recommendations = recommendations.replace('\\n', '\n').replace('\\"', '"').replace('\\t', '    ')
                
                if summary and recommendations:
                    logger.info("Regex alap√∫ JSON kinyer√©s sikeres")
                    return summary, recommendations
                
                # Harmadik pr√≥b√°lkoz√°s: manu√°lis feldolgoz√°s
                logger.warning("Regex parsing sikertelen, manu√°lis feldolgoz√°s...")
                return self._parse_ai_response_manually(ai_response)
                
        except Exception as e:
            logger.error(f"Hiba az AI √∂sszefoglal√≥ gener√°l√°sa sor√°n: {str(e)}")
            # Negyedik pr√≥b√°lkoz√°s: fallback summary gener√°l√°s
            logger.info("Fallback summary gener√°l√°s...")
            return self._fallback_summary_generation(analysis_summary)
                
        except Exception as e:
            logger.error(f"Kritikus hiba az AI √∂sszefoglal√≥ gener√°l√°sa sor√°n: {str(e)}")
            
            # V√©gs≈ë fallback: legal√°bb alapinform√°ci√≥kat adjunk
            try:
                # Ha van analysis_summary, haszn√°ljuk a fallback gener√°tort
                if 'analysis_summary' in locals():
                    return self._fallback_summary_generation(analysis_summary)
            except:
                pass
            
            # Ha semmi sem m≈±k√∂dik, √°ltal√°nos hiba√ºzenet
            return (
                "Az AI √∂sszefoglal√≥ gener√°l√°sa sikertelen volt. K√©rlek ellen≈ërizd az OpenAI API kulcsot √©s a kapcsolatot.",
                "A javaslatok automatikus gener√°l√°sa nem siker√ºlt. Tekintsd √°t manu√°lisan az elemz√©si eredm√©nyeket."
            )
    
    def _create_compact_summary(self, data: Union[Dict, List]) -> Dict[str, Any]:
        """
        Kompakt √∂sszefoglal√≥ k√©sz√≠t√©se - maximum 4000 token m√©ret≈±
        """
        try:
            # Normaliz√°ljuk az adatot - lehet dict vagy list
            if isinstance(data, dict):
                if 'results' in data:
                    results = data['results']
                else:
                    results = [data]
            elif isinstance(data, list):
                results = data
            else:
                results = []
            
            # Sz≈±rj√ºk ki a hib√°s eredm√©nyeket
            valid_results = [
                r for r in results 
                if isinstance(r, dict) and 'error' not in r and 'ai_readiness_score' in r
            ]
            
            if not valid_results:
                return {"error": "Nincs √©rv√©nyes elemz√©si eredm√©ny"}
            
            # Kompakt √∂sszefoglal√≥ strukt√∫ra
            summary = {
                "overview": {
                    "urls_analyzed": len(valid_results),
                    "avg_ai_score": round(sum(r.get('ai_readiness_score', 0) for r in valid_results) / len(valid_results), 1)
                },
                "urls": []
            }
            
            # Minden URL kompakt √∂sszefoglal√≥ja (max 3 URL r√©szletesen)
            for result in valid_results[:3]:  # Limit√°ljuk 3 URL-re a m√©ret miatt
                url_summary = self._extract_url_essentials(result)
                summary["urls"].append(url_summary)
            
            # Ha t√∂bb mint 3 URL van, csak az alapokat adjuk hozz√°
            if len(valid_results) > 3:
                summary["additional_urls"] = []
                for result in valid_results[3:]:
                    summary["additional_urls"].append({
                        "url": result.get("url", "N/A"),
                        "score": result.get("ai_readiness_score", 0),
                        "main_issues": self._get_top_issues(result)[:2]  # Csak 2 f≈ë probl√©ma
                    })
            
            # Glob√°lis statisztik√°k
            all_scores = [r.get('ai_readiness_score', 0) for r in valid_results]
            summary["statistics"] = {
                "min_score": min(all_scores),
                "max_score": max(all_scores),
                "excellent": len([s for s in all_scores if s >= 85]),
                "good": len([s for s in all_scores if 60 <= s < 85]),
                "poor": len([s for s in all_scores if s < 40])
            }
            
            # Top probl√©m√°k √∂sszes√≠t√©se
            all_issues = []
            for r in valid_results:
                all_issues.extend(self._get_top_issues(r))
            
            # Probl√©ma gyakoris√°g
            issue_freq = {}
            for issue in all_issues:
                issue_freq[issue] = issue_freq.get(issue, 0) + 1
            
            # Top 5 leggyakoribb probl√©ma
            summary["common_issues"] = sorted(
                issue_freq.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:5]
            
            return summary
            
        except Exception as e:
            logger.error(f"Kompakt √∂sszefoglal√≥ k√©sz√≠t√©se sikertelen: {str(e)}")
            return {"error": str(e)}
    
    def _extract_url_essentials(self, result: Dict) -> Dict[str, Any]:
        """
        Egy URL l√©nyegi adatainak kinyer√©se - kompakt form√°ban
        """
        url = result.get("url", "N/A")
        score = result.get("ai_readiness_score", 0)
        
        # Meta adatok
        meta = result.get("meta_and_headings", {})
        meta_summary = {
            "title_ok": meta.get("title_optimal", False),
            "desc_ok": meta.get("description_optimal", False),
            "h1_count": meta.get("h1_count", 0),
            "social": meta.get("has_og_tags", False) or meta.get("has_twitter_card", False)
        }
        
        # Schema
        schema = result.get("schema", {})
        schema_count = 0
        if isinstance(schema.get("count"), dict):
            schema_count = sum(schema["count"].values())
        elif isinstance(schema.get("count"), (int, float)):
            schema_count = schema["count"]
        
        # Content
        content = result.get("content_quality", {})
        content_score = content.get("overall_quality_score", 0) if not content.get("error") else 0
        word_count = content.get("readability", {}).get("word_count", 0) if not content.get("error") else 0
        
        # Platforms
        platforms = result.get("platform_analysis", {})
        platform_scores = {}
        if platforms and not platforms.get("error"):
            for p_name in ["chatgpt", "claude", "gemini", "bing_chat"]:
                if p_name in platforms and isinstance(platforms[p_name], dict):
                    platform_scores[p_name] = round(platforms[p_name].get("compatibility_score", 0), 0)
        
        # PageSpeed
        psi = result.get("pagespeed_insights", {})
        perf_mobile = psi.get("mobile", {}).get("performance", 0) if psi else 0
        
        # Technical
        tech_ok = (
            result.get("robots_txt", {}).get("can_fetch", False) and
            result.get("sitemap", {}).get("exists", False) and
            result.get("mobile_friendly", {}).get("has_viewport", False)
        )
        
        return {
            "url": url,
            "ai_score": score,
            "level": self._get_level(score),
            "meta": meta_summary,
            "schema_count": schema_count,
            "content_score": round(content_score, 0),
            "word_count": word_count,
            "platforms": platform_scores,
            "mobile_perf": perf_mobile,
            "tech_ok": tech_ok,
            "top_issues": self._get_top_issues(result)[:3],  # Max 3 issue
            "quick_wins": self._get_quick_wins(result)[:2]   # Max 2 quick win
        }
    
    def _ultra_compact_summary(self, summary: Dict) -> Dict[str, Any]:
        """
        Ultra kompakt √∂sszefoglal√≥ ha m√©g mindig t√∫l nagy
        """
        # Csak az els≈ë URL r√©szletes adatai + statisztik√°k
        ultra_compact = {
            "overview": summary.get("overview", {}),
            "first_url": summary.get("urls", [{}])[0] if summary.get("urls") else {},
            "other_urls_count": len(summary.get("urls", [])) - 1 + len(summary.get("additional_urls", [])),
            "statistics": summary.get("statistics", {}),
            "top_issues": summary.get("common_issues", [])[:3]
        }
        
        # Tov√°bbi URL-ek csak score-ral
        if len(summary.get("urls", [])) > 1:
            ultra_compact["other_scores"] = [
                {"url": u.get("url", "")[:50], "score": u.get("ai_score", 0)} 
                for u in summary.get("urls", [])[1:3]
            ]
        
        return ultra_compact
    
    def _get_level(self, score: float) -> str:
        """Szint meghat√°roz√°s"""
        if score >= 85: return "Kiv√°l√≥"
        if score >= 60: return "J√≥"
        if score >= 40: return "K√∂zepes"
        return "Gyenge"
    
    def _get_top_issues(self, result: Dict) -> List[str]:
        """Top probl√©m√°k azonos√≠t√°sa"""
        issues = []
        
        # Meta
        meta = result.get("meta_and_headings", {})
        if not meta.get("title_optimal"):
            issues.append("Title nem optim√°lis")
        if not meta.get("description_optimal"):
            issues.append("Description hi√°nyzik/rossz")
        if meta.get("h1_count", 0) != 1:
            issues.append(f"H1 probl√©ma ({meta.get('h1_count', 0)} db)")
        
        # Schema
        schema = result.get("schema", {})
        schema_count = 0
        if isinstance(schema.get("count"), dict):
            schema_count = sum(schema["count"].values())
        elif isinstance(schema.get("count"), (int, float)):
            schema_count = schema["count"]
        
        if schema_count == 0:
            issues.append("Nincs Schema markup")
        
        # Content
        content = result.get("content_quality", {})
        if not content.get("error"):
            word_count = content.get("readability", {}).get("word_count", 0)
            if word_count < 300:
                issues.append(f"Kev√©s tartalom ({word_count} sz√≥)")
        
        # Technical
        if not result.get("robots_txt", {}).get("can_fetch"):
            issues.append("Robots.txt tilt√°s")
        if not result.get("sitemap", {}).get("exists"):
            issues.append("Nincs sitemap")
        if not result.get("mobile_friendly", {}).get("has_viewport"):
            issues.append("Nincs mobile viewport")
        
        # Performance
        psi = result.get("pagespeed_insights", {})
        if psi:
            mobile_perf = psi.get("mobile", {}).get("performance", 100)
            if mobile_perf < 50:
                issues.append(f"Gyenge mobil sebess√©g ({mobile_perf})")
        
        return issues
    
    def _get_quick_wins(self, result: Dict) -> List[str]:
        """Gyors jav√≠t√°sok"""
        wins = []
        
        meta = result.get("meta_and_headings", {})
        if not meta.get("title_optimal"):
            wins.append("Title tag optimaliz√°l√°s (30-60 karakter)")
        
        if not meta.get("description"):
            wins.append("Meta description hozz√°ad√°sa")
        
        schema = result.get("schema", {})
        schema_count = 0
        if isinstance(schema.get("count"), dict):
            schema_count = sum(schema["count"].values())
        
        if schema_count == 0:
            wins.append("FAQ vagy Article Schema hozz√°ad√°sa")
        
        if not meta.get("has_og_tags"):
            wins.append("Open Graph tagek hozz√°ad√°sa")
        
        if not result.get("sitemap", {}).get("exists"):
            wins.append("XML sitemap l√©trehoz√°sa")
        
        return wins[:3]  # Max 3
    
    def _fallback_summary_generation(self, analysis_summary: Dict) -> Tuple[str, str]:
        """
        Fallback √∂sszefoglal√≥ gener√°l√°s ha az AI nem ad valid JSON-t
        """
        try:
            overview = analysis_summary.get("overview", {})
            urls = analysis_summary.get("urls", [])
            stats = analysis_summary.get("statistics", {})
            issues = analysis_summary.get("common_issues", [])
            
            # √ñsszefoglal√≥ gener√°l√°sa
            summary_parts = []
            
            # √Åttekint√©s
            avg_score = overview.get("avg_ai_score", 0)
            url_count = overview.get("urls_analyzed", 0)
            
            summary_parts.append(f"Az elemz√©s {url_count} URL vizsg√°lat√°t tartalmazza, √°tlagos AI Readiness Score: {avg_score}/100.")
            
            # Szint √©rt√©kel√©s
            if avg_score >= 85:
                level_text = "kiv√°l√≥, az oldalak k√©szen √°llnak az AI platformokra"
            elif avg_score >= 60:
                level_text = "j√≥, de van m√©g fejleszt√©si potenci√°l"
            elif avg_score >= 40:
                level_text = "k√∂zepes, jelent≈ës optimaliz√°ci√≥ra van sz√ºks√©g"
            else:
                level_text = "gyenge, s√ºrg≈ës beavatkoz√°s sz√ºks√©ges"
            
            summary_parts.append(f"Az √°tlagos teljes√≠tm√©ny {level_text}.")
            
            # Statisztik√°k
            if stats:
                summary_parts.append(f"\nStatisztik√°k: {stats.get('excellent', 0)} kiv√°l√≥, {stats.get('good', 0)} j√≥, {stats.get('poor', 0)} gyenge teljes√≠tm√©ny≈± oldal.")
                summary_parts.append(f"Legjobb eredm√©ny: {stats.get('max_score', 0)}/100, leggyeng√©bb: {stats.get('min_score', 0)}/100.")
            
            # URL r√©szletek
            if urls:
                summary_parts.append("\nR√©szletes eredm√©nyek:")
                for i, url_data in enumerate(urls[:3], 1):
                    url = url_data.get("url", "N/A")
                    score = url_data.get("ai_score", 0)
                    level = url_data.get("level", "N/A")
                    top_issues = url_data.get("top_issues", [])
                    
                    summary_parts.append(f"\n{i}. {url}: {score}/100 ({level})")
                    if top_issues:
                        summary_parts.append(f"   F≈ëbb probl√©m√°k: {', '.join(top_issues[:3])}")
            
            # Leggyakoribb probl√©m√°k
            if issues:
                summary_parts.append("\n\nLeggyakoribb probl√©m√°k az √∂sszes oldalon:")
                for issue, count in issues[:5]:
                    summary_parts.append(f"- {issue}: {count} alkalommal")
            
            summary = " ".join(summary_parts)
            
            # Javaslatok gener√°l√°sa
            rec_parts = []
            
            rec_parts.append("Prioritiz√°lt fejleszt√©si javaslatok:\n")
            
            # Kritikus jav√≠t√°sok
            if avg_score < 40:
                rec_parts.append("KRITIKUS PRIORIT√ÅS:")
                rec_parts.append("1. Schema.org markup azonnali implement√°l√°sa minden oldalon")
                rec_parts.append("2. Meta title √©s description optimaliz√°l√°s (30-60 √©s 120-160 karakter)")
                rec_parts.append("3. Mobile viewport meta tag hozz√°ad√°sa")
            
            # √Åltal√°nos javaslatok az issues alapj√°n
            rec_parts.append("\nF≈ëbb teend≈ëk a probl√©m√°k alapj√°n:")
            
            issue_solutions = {
                "Title nem optim√°lis": "Optimaliz√°ld a title tageket 30-60 karakterre, haszn√°lj kulcsszavakat",
                "Description hi√°nyzik/rossz": "Adj hozz√° meta description-t 120-160 karakterrel",
                "Nincs Schema markup": "Implement√°lj FAQ vagy Article schema-t JSON-LD form√°tumban",
                "Kev√©s tartalom": "B≈ëv√≠tsd a tartalmat minimum 300-500 sz√≥ra",
                "Nincs sitemap": "Hozz l√©tre XML sitemap-et √©s add hozz√° a robots.txt-hez",
                "Nincs mobile viewport": "Add hozz√°: <meta name='viewport' content='width=device-width, initial-scale=1'>",
                "H1 probl√©ma": "Haszn√°lj pontosan 1 db H1 c√≠msort oldalank√©nt"
            }
            
            rec_num = 1
            for issue, count in issues[:5]:
                if issue in issue_solutions:
                    rec_parts.append(f"{rec_num}. {issue_solutions[issue]} (√ârint: {count} oldalt)")
                    rec_num += 1
            
            # Quick wins
            if urls and urls[0].get("quick_wins"):
                rec_parts.append("\n\nGyors eredm√©nyek (Quick Wins):")
                for win in urls[0]["quick_wins"][:3]:
                    rec_parts.append(f"- {win}")
            
            # Platform-specifikus
            rec_parts.append("\n\nPlatform-specifikus optimaliz√°ci√≥:")
            rec_parts.append("- ChatGPT: Struktur√°lt tartalom, Q&A form√°tum, l√©p√©senk√©nti √∫tmutat√≥k")
            rec_parts.append("- Claude: R√©szletes kontextus, hivatkoz√°sok, hossz√∫ form√°tum√∫ tartalom")
            rec_parts.append("- Gemini: Friss inform√°ci√≥k, multim√©dia, schema markup")
            rec_parts.append("- Bing Chat: K√ºls≈ë forr√°sok, id≈ëszer≈± tartalom, fact-checking")
            
            # V√°rhat√≥ javul√°s
            improvement = min(100 - avg_score, 30)
            rec_parts.append(f"\n\nV√°rhat√≥ AI Score javul√°s a javaslatok implement√°l√°sa ut√°n: +{improvement} pont")
            
            recommendations = "\n".join(rec_parts)
            
            return summary, recommendations
            
        except Exception as e:
            logger.error(f"Fallback generation hiba: {str(e)}")
            return (
                "Az automatikus √∂sszefoglal√≥ gener√°l√°s sikertelen volt.",
                "K√©rlek ellen≈ërizd manu√°lisan az elemz√©si eredm√©nyeket."
            )
    
    def _parse_ai_response_manually(self, response: str) -> Tuple[str, str]:
        """
        Robusztus manu√°lis feldolgoz√°s k√ºl√∂nb√∂z≈ë AI v√°lasz form√°tumokhoz
        """
        try:
            import re
            
            logger.info(f"Manu√°lis parsing ind√≠t√°sa, v√°lasz hossza: {len(response)} karakter")
            
            # K√ºl√∂nb√∂z≈ë elv√°laszt√≥k keres√©se
            patterns = [
                # JSON-szer≈± strukt√∫ra id√©z≈ëjelek n√©lk√ºl
                (r'summary\s*:\s*(.*?)(?:recommendations|javaslat|$)', r'recommendations\s*:\s*(.*)'),
                # Sz√°mozott lista
                (r'1\.\s*(?:√ñSSZEFOGLAL√ì|Summary|√ñsszefoglal√≥)[:\s]*(.*?)(?:2\.|recommendations|javaslat|$)', 
                 r'2\.\s*(?:JAVASLATOK|Recommendations|Javaslatok)[:\s]*(.*)'),
                # Markdown fejl√©cek
                (r'#+\s*(?:√ñsszefoglal√≥|Summary|√ñSSZEFOGLAL√ì)(.*?)(?:#+\s*(?:Javaslatok|Recommendations)|$)', 
                 r'#+\s*(?:Javaslatok|Recommendations|JAVASLATOK)(.*)'),
                # Nagybet≈±s elv√°laszt√≥k
                (r'√ñSSZEFOGLAL√ì[:\s]*(.*?)(?:JAVASLATOK|RECOMMENDATIONS|$)', 
                 r'(?:JAVASLATOK|RECOMMENDATIONS)[:\s]*(.*)'),
                # B√°rmilyen sz√∂veg "summary" √©s "recommendations" k√∂z√∂tt
                (r'(?:summary|√∂sszefoglal√≥)[:\s]*(.*?)(?:recommendations|javaslatok|aj√°nl√°sok)', 
                 r'(?:recommendations|javaslatok|aj√°nl√°sok)[:\s]*(.*)'),
            ]
            
            summary = None
            recommendations = None
            
            # Pr√≥b√°ljuk meg az √∂sszes pattern-t
            for sum_pattern, rec_pattern in patterns:
                if not summary:
                    sum_match = re.search(sum_pattern, response, re.IGNORECASE | re.DOTALL)
                    if sum_match:
                        summary = sum_match.group(1).strip()
                        logger.info(f"Summary pattern match tal√°lat, hossz: {len(summary)}")
                
                if not recommendations:
                    rec_match = re.search(rec_pattern, response, re.IGNORECASE | re.DOTALL)
                    if rec_match:
                        recommendations = rec_match.group(1).strip()
                        logger.info(f"Recommendations pattern match tal√°lat, hossz: {len(recommendations)}")
                
                if summary and recommendations:
                    break
            
            # Ha m√©g mindig nincs, pr√≥b√°ljuk meg a v√°lasz felez√©s√©vel
            if not summary or not recommendations:
                logger.info("Pattern matching sikertelen, v√°lasz felez√©se...")
                # Keress√ºnk term√©szetes t√∂r√©spontot
                break_points = [
                    response.find("Javaslatok"),
                    response.find("JAVASLATOK"),
                    response.find("Recommendations"),
                    response.find("recommendations"),
                    response.find("2."),
                    len(response) // 2  # V√©gs≈ë esetben felezz√ºk
                ]
                
                # Els≈ë valid t√∂r√©spont
                break_point = next((p for p in break_points if p > 100), len(response) // 2)
                logger.info(f"T√∂r√©spont: {break_point}")
                
                if not summary:
                    summary = response[:break_point].strip()
                if not recommendations:
                    recommendations = response[break_point:].strip()
            
            # Tiszt√≠t√°s
            # Elt√°vol√≠tjuk a felesleges karaktereket √©s form√°z√°sokat
            if summary:
                # JSON maradv√°nyok
                summary = summary.replace('"', '').replace('{', '').replace('}', '').replace("'", "")
                summary = summary.replace('\\n', '\n').replace('\\t', ' ')
                # Markdown
                summary = re.sub(r'[#*`]', '', summary)
                # T√∂bbsz√∂r√∂s sz√≥k√∂z√∂k
                summary = re.sub(r'\s+', ' ', summary)
                # C√≠mek √©s kulcsszavak elt√°vol√≠t√°sa
                summary = re.sub(r'^(summary|√∂sszefoglal√≥)[:\s]*', '', summary, flags=re.IGNORECASE)
                summary = summary.strip()
            
            if recommendations:
                # JSON maradv√°nyok
                recommendations = recommendations.replace('"', '').replace('{', '').replace('}', '').replace("'", "")
                recommendations = recommendations.replace('\\n', '\n').replace('\\t', ' ')
                # Markdown
                recommendations = re.sub(r'[#*`]', '', recommendations)
                # T√∂bbsz√∂r√∂s sz√≥k√∂z√∂k
                recommendations = re.sub(r'\s+', ' ', recommendations)
                # C√≠mek √©s kulcsszavak elt√°vol√≠t√°sa
                recommendations = re.sub(r'^(recommendations|javaslatok|aj√°nl√°sok)[:\s]*', '', recommendations, flags=re.IGNORECASE)
                recommendations = recommendations.strip()
            
            # Ellen≈ërz√©s
            if summary and len(summary) > 50:
                logger.info("Manu√°lis parsing sikeres")
                return (summary, recommendations or "A javaslatok automatikus gener√°l√°sa r√©szben sikertelen. K√©rlek tekintsd √°t manu√°lisan az elemz√©si eredm√©nyeket.")
            else:
                # Ha t√∫l r√∂vid, haszn√°ljuk a teljes v√°laszt
                logger.warning("Manu√°lis parsing nem adott megfelel≈ë eredm√©nyt, teljes v√°lasz haszn√°lata")
                return (
                    response[:1500] if len(response) > 1500 else response,
                    "A javaslatok automatikus gener√°l√°sa sikertelen. K√©rlek tekintsd √°t manu√°lisan az elemz√©si eredm√©nyeket √©s k√©sz√≠ts optimaliz√°l√°si tervet."
                )
            
        except Exception as e:
            logger.error(f"Manu√°lis parsing kritikus hiba: {str(e)}")
            # V√©gs≈ë fallback - legal√°bb valami inform√°ci√≥t adjunk
            return (
                response[:1000] if len(response) > 1000 else response,
                "Automatikus javaslat gener√°l√°s sikertelen. Manu√°lis elemz√©s sz√ºks√©ges."
            )

def generate_ai_summary_from_file(json_file_path: str) -> Tuple[str, str]:
    """
    Seg√©df√ºggv√©ny: AI √∂sszefoglal√≥ gener√°l√°sa JSON f√°jlb√≥l
    
    Args:
        json_file_path: A JSON f√°jl el√©r√©si √∫tja
        
    Returns:
        Tuple[str, str]: (√∂sszefoglal√≥, javaslatok)
    """
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        generator = AISummaryGenerator()
        return generator.generate_summary_and_recommendations(data)
        
    except FileNotFoundError:
        logger.error(f"JSON f√°jl nem tal√°lhat√≥: {json_file_path}")
        return "A JSON f√°jl nem tal√°lhat√≥.", "K√©rlek, futtasd le el≈ëbb az elemz√©st."
    except Exception as e:
        logger.error(f"Hiba a f√°jl feldolgoz√°sa sor√°n: {str(e)}")
        return "Hiba t√∂rt√©nt a f√°jl feldolgoz√°sa sor√°n.", "Ellen≈ërizd a f√°jl form√°tum√°t √©s pr√≥b√°ld √∫jra."

if __name__ == "__main__":
    # Teszt futtat√°s
    # P√©lda: lista form√°tum√∫ input (ahogy a val√≥ √©letben is √©rkezik)
    test_data = [
        {
            "url": "https://example.com",
            "ai_readiness_score": 75.5,
            "meta_and_headings": {
                "title": "Test Title - Example Site",
                "title_length": 25,
                "title_optimal": False,
                "description": "This is a test description for the example website",
                "description_length": 50,
                "description_optimal": False,
                "h1_count": 1,
                "heading_hierarchy_valid": True,
                "has_og_tags": True,
                "has_twitter_card": False,
                "headings": {"h1": 1, "h2": 3, "h3": 5}
            },
            "schema": {
                "count": {"Article": 1, "BreadcrumbList": 1},
                "has_breadcrumbs": True,
                "validation_status": "enhanced",
                "schema_completeness_score": 85
            },
            "content_quality": {
                "overall_quality_score": 72,
                "readability": {
                    "word_count": 500,
                    "readability_score": 80
                },
                "keyword_analysis": {
                    "vocabulary_richness": 0.65
                }
            },
            "platform_analysis": {
                "chatgpt": {
                    "compatibility_score": 75,
                    "hybrid_score": 78,
                    "ai_score": 80
                },
                "claude": {
                    "compatibility_score": 70,
                    "hybrid_score": 72,
                    "ai_score": 74
                }
            },
            "robots_txt": {"can_fetch": True},
            "sitemap": {"exists": True},
            "mobile_friendly": {"has_viewport": True},
            "pagespeed_insights": {
                "mobile": {"performance": 65, "seo": 88}
            }
        },
        {
            "url": "https://example2.com",
            "ai_readiness_score": 45.2,
            "meta_and_headings": {
                "title": "Short",
                "title_length": 5,
                "title_optimal": False,
                "description": None,
                "description_length": 0,
                "description_optimal": False,
                "h1_count": 0,
                "heading_hierarchy_valid": False,
                "has_og_tags": False,
                "has_twitter_card": False
            },
            "schema": {
                "count": {},
                "has_breadcrumbs": False,
                "validation_status": "standard"
            },
            "content_quality": {
                "overall_quality_score": 35,
                "readability": {
                    "word_count": 150,
                    "readability_score": 45
                }
            },
            "robots_txt": {"can_fetch": True},
            "sitemap": {"exists": False},
            "mobile_friendly": {"has_viewport": False}
        }
    ]
    
    try:
        generator = AISummaryGenerator()
        print("üöÄ AI Summary Generator teszt ind√≠t√°sa...")
        print(f"üìä {len(test_data)} URL elemz√©se")
        
        summary, recommendations = generator.generate_summary_and_recommendations(test_data)
        
        print("\n" + "="*60)
        print("üìù √ñSSZEFOGLAL√ì:")
        print("="*60)
        print(summary)
        
        print("\n" + "="*60)
        print("üí° JAVASLATOK:")
        print("="*60)
        print(recommendations)
        
        print("\n‚úÖ Teszt sikeresen lefutott!")
        
    except Exception as e:
        print(f"‚ùå Teszt hiba: {str(e)}")
        import traceback
        traceback.print_exc()