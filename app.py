import streamlit as st
import json
import os
from main import analyze_urls_enhanced, GEOAnalyzer
from report import generate_html_report, generate_csv_export
from advanced_reporting import AdvancedReportGenerator  
import time

st.set_page_config(
    page_title="Enhanced GEO Analyzer",
    page_icon="üöÄ",
    layout="wide"
)

st.title("üöÄ Enhanced GEO AI Readiness Analyzer")
st.markdown("**Generative Engine Optimization** elemz√©s eszk√∂z - AI-Enhanced verzi√≥")

# Sidebar be√°ll√≠t√°sok
st.sidebar.header("‚öôÔ∏è Be√°ll√≠t√°sok")

# API kulcs be√°ll√≠t√°s
api_key = st.sidebar.text_input(
    "Google PageSpeed API kulcs:",
    type="password",
    help="Opcion√°lis - PageSpeed Insights-hoz sz√ºks√©ges"
)

# Enhanced be√°ll√≠t√°sok
st.sidebar.subheader("ü§ñ AI Enhancement")

use_ai_evaluation = st.sidebar.checkbox(
    "AI tartalom √©rt√©kel√©s",
    value=True,
    help="AI-alap√∫ tartalom min≈ës√©g √©rt√©kel√©s bekapcsol√°sa"
)

use_cache = st.sidebar.checkbox(
    "Intelligens cache",
    value=True,
    help="Eredm√©nyek cache-el√©se a gyorsabb feldolgoz√°s√©rt"
)

force_refresh = st.sidebar.checkbox(
    "Cache kihagy√°sa",
    value=False,
    help="Minden URL √∫jraelemz√©se cache mell≈ëz√©s√©vel"
)

# Elemz√©si opci√≥k
st.sidebar.subheader("üìä Elemz√©si be√°ll√≠t√°sok")

skip_pagespeed = st.sidebar.checkbox(
    "PageSpeed √°tugr√°sa", 
    value=not bool(api_key),
    help="Gyorsabb elemz√©s PageSpeed n√©lk√ºl"
)

parallel_processing = st.sidebar.checkbox(
    "P√°rhuzamos feldolgoz√°s", 
    value=True,
    help="T√∂bb URL egyidej≈± elemz√©se (gyorsabb)"
)

max_workers = st.sidebar.slider(
    "P√°rhuzamos sz√°lak sz√°ma",
    min_value=1,
    max_value=5,
    value=2,
    help="T√∂bb sz√°l = gyorsabb, de t√∂bb terhel√©s"
)

st.sidebar.subheader("üöÄ Fejlett funkci√≥k")

generate_advanced_reports = st.sidebar.checkbox(
    "Fejlett jelent√©sek gener√°l√°sa",
    value=True,
    help="Executive, technikai √©s action plan jelent√©sek"
)

report_type = st.sidebar.selectbox(
    "Jelent√©s t√≠pusa:",
    ["executive", "technical", "action_plan", "competitor"],
    help="Milyen t√≠pus√∫ r√©szletes jelent√©st szeretn√©l?"
)

enable_ai_fixes = st.sidebar.checkbox(
    "Automatikus jav√≠t√°si javaslatok",
    value=True,
    help="AI-alap√∫ optimaliz√°l√°si javaslatok gener√°l√°sa"
)

platform_focus = st.sidebar.multiselect(
    "AI platform f√≥kusz:",
    ["ChatGPT", "Claude", "Gemini", "Bing Chat"],
    default=["ChatGPT", "Claude"],
    help="Melyik AI platformokra optimaliz√°lj?"
)

# Cache st√°tusz megjelen√≠t√©se
if use_cache:
    st.sidebar.subheader("üíæ Cache √°llapot")
    if st.sidebar.button("Cache statisztik√°k"):
        try:
            analyzer = GEOAnalyzer(use_cache=True)
            cache_stats = analyzer.get_cache_stats()
            if cache_stats.get('cache_enabled'):
                st.sidebar.success(f"Cache f√°jlok: {cache_stats.get('total_files', 0)}")
                st.sidebar.info(f"M√©ret: {cache_stats.get('total_size_mb', 0)} MB")
            else:
                st.sidebar.warning("Cache nem el√©rhet≈ë")
        except Exception as e:
            st.sidebar.error(f"Cache hiba: {e}")
    
    if st.sidebar.button("Cache tiszt√≠t√°s"):
        try:
            analyzer = GEOAnalyzer(use_cache=True)
            cleanup_result = analyzer.cleanup_cache()
            st.sidebar.success(f"T√∂r√∂lve: {cleanup_result.get('cleaned_files', 0)} f√°jl")
        except Exception as e:
            st.sidebar.error(f"Tiszt√≠t√°s hiba: {e}")

# F≈ëoldal
col1, col2 = st.columns([2, 1])

with col1:
    st.header("üìù URL-ek megad√°sa")
    
    # URL input m√≥dok
    input_method = st.radio(
        "URL megad√°s m√≥dja:",
        ["Sz√∂veges lista", "F√°jl felt√∂lt√©s"]
    )
    
    if input_method == "Sz√∂veges lista":
        urls_text = st.text_area(
            "URL-ek (soronk√©nt egy URL):",
            height=200,
            placeholder="https://example.com\nhttps://another-site.com\nhttps://third-site.com"
        )
        url_list = [url.strip() for url in urls_text.split('\n') if url.strip()]
    
    else:
        uploaded_file = st.file_uploader(
            "URL lista felt√∂lt√©se",
            type=['txt', 'csv'],
            help="Soronk√©nt egy URL a f√°jlban"
        )
        
        url_list = []
        if uploaded_file:
            content = str(uploaded_file.read(), "utf-8")
            url_list = [url.strip() for url in content.split('\n') if url.strip()]
    
    # URL el≈ën√©zet
    if url_list:
        st.success(f"‚úÖ {len(url_list)} URL bet√∂ltve")
        with st.expander("URL lista el≈ën√©zete"):
            for i, url in enumerate(url_list[:10], 1):
                ai_icon = "ü§ñ" if use_ai_evaluation else "üìä"
                cache_icon = "üíæ" if use_cache else "üîÑ"
                st.text(f"{i}. {ai_icon}{cache_icon} {url}")
            if len(url_list) > 10:
                st.text(f"... √©s m√©g {len(url_list) - 10} URL")

with col2:
    st.header("üìä El≈ëz≈ë elemz√©sek")
    
    # Enhanced jelent√©sek list√°z√°sa
    json_files = [f for f in os.listdir('.') if f.endswith('_report.json')]
    enhanced_files = [f for f in json_files if 'enhanced' in f]
    html_files = [f for f in os.listdir('.') if f.endswith('.html') and 'report' in f]
    
    if enhanced_files:
        st.subheader("ü§ñ Enhanced jelent√©sek:")
        for file in enhanced_files[:5]:
            if st.button(f"üìÅ {file}", key=f"enhanced_{file}"):
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # Enhanced adatok megjelen√≠t√©se
                valid_results = [r for r in data if 'ai_readiness_score' in r and 'error' not in r]
                ai_enhanced_count = len([r for r in valid_results if r.get('ai_content_evaluation')])
                
                st.json({
                    "total_sites": len(data),
                    "valid_results": len(valid_results),
                    "ai_enhanced_results": ai_enhanced_count,
                    "avg_score": sum(r['ai_readiness_score'] for r in valid_results) / len(valid_results) if valid_results else 0
                })
    
    if json_files and not enhanced_files:
        st.subheader("üìä Standard jelent√©sek:")
        for file in json_files[:5]:
            if st.button(f"üìÅ {file}", key=f"json_{file}"):
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                st.json(data[:2] if isinstance(data, list) else data)  # Csak mint√°t mutatunk
    
    if html_files:
        st.subheader("üìÑ HTML jelent√©sek:")
        for file in html_files[:5]:
            st.markdown(f"üìÑ [{file}](./{file})")

# Elemz√©s ind√≠t√°sa
st.header("üöÄ Enhanced Elemz√©s ind√≠t√°sa")

# Elemz√©s konfigur√°ci√≥j√°nak megjelen√≠t√©se
if url_list:
    config_col1, config_col2, config_col3 = st.columns(3)
    
    with config_col1:
        st.metric("URL-ek sz√°ma", len(url_list))
        st.metric("P√°rhuzamos sz√°lak", max_workers if parallel_processing else 1)
    
    with config_col2:
        st.metric("AI Enhancement", "‚úÖ" if use_ai_evaluation else "‚ùå")
        st.metric("Cache", "‚úÖ" if use_cache else "‚ùå")
    
    with config_col3:
        st.metric("PageSpeed", "‚ùå" if skip_pagespeed else "‚úÖ")
        st.metric("Force Refresh", "‚úÖ" if force_refresh else "‚ùå")

if st.button("‚ñ∂Ô∏è Enhanced Elemz√©s kezd√©se", type="primary", disabled=not bool(url_list)):
    if not url_list:
        st.error("‚ùå Nem adt√°l meg URL-eket!")
    else:
        # Progress bar √©s status
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # Id≈ëm√©r√©s kezd√©se
            start_time = time.time()
            
            status_text.text("üöÄ Enhanced elemz√©s inicializ√°l√°sa...")
            progress_bar.progress(10)
            
            # F√°jln√©v gener√°l√°s timestamp-pel
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            json_filename = f"geo_enhanced_analysis_{timestamp}.json"
            html_filename = f"geo_enhanced_report_{timestamp}.html"
            
            status_text.text("ü§ñ Enhanced URL-ek elemz√©se folyamatban...")
            progress_bar.progress(20)
            
            # Enhanced elemz√©s futtat√°sa
            analyze_urls_enhanced(
                url_list=url_list,
                api_key=api_key if not skip_pagespeed else None,
                output_file=json_filename,
                parallel=parallel_processing,
                skip_pagespeed=skip_pagespeed,
                max_workers=max_workers if parallel_processing else 1,
                use_cache=use_cache,
                use_ai=use_ai_evaluation,
                force_refresh=force_refresh
            )
            
            progress_bar.progress(70)
            status_text.text("üìã Enhanced HTML jelent√©s gener√°l√°sa...")
            
            # HTML jelent√©s
            generate_html_report(json_filename, html_filename)
            
            progress_bar.progress(90)
            status_text.text("üìä CSV export...")
            
            # CSV export
            csv_filename = f"geo_enhanced_export_{timestamp}.csv"
            generate_csv_export(json_filename, csv_filename)
            
            progress_bar.progress(100)
            
            # Sikeres befejez√©s
            elapsed_time = time.time() - start_time
            status_text.text(f"‚úÖ Enhanced elemz√©s befejezve! ({elapsed_time:.1f} m√°sodperc)")
            
            # Download gombok
            col1, col2, col3 = st.columns(3)
            
            with col1:
                with open(json_filename, 'rb') as f:
                    st.download_button(
                        "ü§ñ Enhanced JSON",
                        f,
                        file_name=json_filename,
                        mime="application/json"
                    )
            
            with col2:
                with open(html_filename, 'rb') as f:
                    st.download_button(
                        "üìÑ Enhanced HTML",
                        f,
                        file_name=html_filename,
                        mime="text/html"
                    )
            
            with col3:
                with open(csv_filename, 'rb') as f:
                    st.download_button(
                        "üìà Enhanced CSV",
                        f,
                        file_name=csv_filename,
                        mime="text/csv"
                    )
            
            # Enhanced √∂sszefoglal√≥ megjelen√≠t√©se
            with open(json_filename, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            st.header("üìà Enhanced √ñsszefoglal√≥")
            
            col1, col2, col3, col4, col5 = st.columns(5)
            
            valid_results = [r for r in results if 'ai_readiness_score' in r and 'error' not in r]
            ai_enhanced_results = [r for r in valid_results if r.get('ai_content_evaluation')]
            schema_enhanced_results = [r for r in valid_results if r.get('schema', {}).get('validation_status') == 'enhanced']
            
            with col1:
                st.metric("Elemzett oldalak", len(results))
            
            with col2:
                if valid_results:
                    avg_score = sum(r['ai_readiness_score'] for r in valid_results) / len(valid_results)
                    st.metric("√Åtlagos AI Score", f"{avg_score:.1f}/100")
            
            with col3:
                st.metric("AI Enhanced", len(ai_enhanced_results))
            
            with col4:
                st.metric("Schema Enhanced", len(schema_enhanced_results))
            
            with col5:
                excellent_count = sum(1 for r in valid_results if r['ai_readiness_score'] >= 70)
                st.metric("Kiv√°l√≥ oldalak", excellent_count)
            
            # R√©szletes enhanced eredm√©nyek
            if valid_results:
                st.subheader("üèÜ Enhanced Legjobb eredm√©nyek")
                sorted_results = sorted(valid_results, key=lambda x: x['ai_readiness_score'], reverse=True)
                
                for i, result in enumerate(sorted_results[:5], 1):
                    score = result['ai_readiness_score']
                    url = result['url']
                    
                    # Enhanced ikonok
                    ai_icon = "ü§ñ" if result.get('ai_content_evaluation') else "üìä"
                    schema_icon = "üèóÔ∏è" if result.get('schema', {}).get('validation_status') == 'enhanced' else "üìã"
                    cache_icon = "üíæ" if result.get('cached') else "üîÑ"
                    
                    # Sz√≠n a score alapj√°n
                    if score >= 80:
                        color = "üü¢"
                    elif score >= 60:
                        color = "üü°"
                    else:
                        color = "üî¥"
                    
                    st.write(f"{color} **{i}.** {ai_icon}{schema_icon}{cache_icon} {url} - **{score}/100**")
                    
                    # Enhanced r√©szletek
                    if result.get('ai_content_evaluation'):
                        ai_overall = result['ai_content_evaluation'].get('overall_ai_score', 0)
                        st.caption(f"   AI Overall Score: {ai_overall:.1f}/100")
                    
                    if result.get('schema', {}).get('validation_status') == 'enhanced':
                        schema_score = result['schema'].get('schema_completeness_score', 0)
                        st.caption(f"   Schema Completeness: {schema_score:.1f}/100")
            
            # Cache statisztik√°k megjelen√≠t√©se
            if use_cache:
                st.subheader("üíæ Cache Teljes√≠tm√©ny")
                try:
                    analyzer = GEOAnalyzer(use_cache=True)
                    cache_stats = analyzer.get_cache_stats()
                    
                    cache_col1, cache_col2, cache_col3 = st.columns(3)
                    with cache_col1:
                        st.metric("Cache f√°jlok", cache_stats.get('total_files', 0))
                    with cache_col2:
                        st.metric("√ârv√©nyes cache", cache_stats.get('valid_files', 0))
                    with cache_col3:
                        st.metric("Cache m√©ret", f"{cache_stats.get('total_size_mb', 0)} MB")
                        
                except Exception as e:
                    st.warning(f"Cache statisztik√°k nem el√©rhet≈ëk: {e}")
            
        except Exception as e:
            st.error(f"‚ùå Hiba t√∂rt√©nt az enhanced elemz√©s sor√°n: {str(e)}")
            status_text.text("‚ùå Enhanced elemz√©s megszak√≠tva")
            import traceback
            st.code(traceback.format_exc())

# Inform√°ci√≥s szekci√≥
with st.expander("‚ÑπÔ∏è Enhanced GEO Analyzer inform√°ci√≥k"):
    st.markdown("""
    ### üöÄ √öjdons√°gok az Enhanced verzi√≥ban:
    
    **ü§ñ AI-alap√∫ tartalom √©rt√©kel√©s:**
    - Val√≥di AI-alap√∫ tartalom min≈ës√©g √©rt√©kel√©s
    - Platform-specifikus AI optimaliz√°l√°s
    - Szemantikai relev√°ncia m√©r√©s
    - Faktualit√°s ellen≈ërz√©s
    
    **üíæ Intelligens Cache rendszer:**
    - Automatikus eredm√©ny cache-el√©s
    - Gyorsabb √∫jrafuttat√°s
    - Intelligent invalidation
    - Cache statisztik√°k √©s tiszt√≠t√°s
    
    **üèóÔ∏è Enhanced Schema valid√°ci√≥:**
    - Google Rich Results Test szimul√°ci√≥
    - Schema effectiveness m√©r√©s
    - Dinamikus schema aj√°nl√°sok
    - AI-alap√∫ schema gener√°l√°s
    
    **üìä Platform-specifikus fejleszt√©sek:**
    - Machine Learning alap√∫ scoring
    - Hibrid pontsz√°m√≠t√°s (heurisztikus + AI)
    - Platform-specifikus A/B testing lehet≈ës√©g
    - Enhanced competitive analysis
    
    ### Mi az a GEO (Generative Engine Optimization)?
    
    A **Generative Engine Optimization** az AI-alap√∫ keres≈ëmotorok (mint ChatGPT, Claude, Gemini) sz√°m√°ra val√≥ optimaliz√°l√°s.
    
    ### Mit ellen≈ëriz az Enhanced alkalmaz√°s?
    
    ‚úÖ **Minden eredeti funkci√≥ plus:**
    - AI-alap√∫ tartalom min≈ës√©g √©rt√©kel√©s
    - Enhanced schema valid√°ci√≥ √©s effectiveness
    - Platform-specifikus AI scoring
    - Intelligent caching √©s performance optimization
    
    ### Hogyan haszn√°ld az Enhanced verzi√≥t?
    
    1. **AI Enhancement:** Kapcsold be a fejlett AI √©rt√©kel√©st
    2. **Cache:** Haszn√°ld a cache-t a gyorsabb elemz√©s√©rt  
    3. **URL-ek:** Add meg az elemezni k√≠v√°nt URL-eket
    4. **Elemz√©s:** Ind√≠tsd el az enhanced elemz√©st
    5. **Eredm√©nyek:** T√∂ltsd le a r√©szletes jelent√©seket
    
    ### Enhanced API kulcsok
    
    Az Enhanced verzi√≥ jelenleg szimul√°lja az AI API h√≠v√°sokat a k√∂lts√©ghat√©konys√°g √©rdek√©ben.
    Val√≥s implement√°ci√≥ban Claude/OpenAI API kulcsok sz√ºks√©gesek.
    """)

# Footer
st.markdown("---")
st.markdown("üöÄ **Enhanced GEO Analyzer** | AI-Powered Analysis | K√©sz√≠tette: Ecsedi Tam√°s | 2025")