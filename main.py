import requests
from urllib.parse import urlparse, quote
from bs4 import BeautifulSoup
import json
from urllib.robotparser import RobotFileParser
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Tuple, List, Optional
import time
from dotenv import load_dotenv
import os
import re

from ai_metrics import AISpecificMetrics
from content_analyzer import ContentQualityAnalyzer
from platform_optimizer import MultiPlatformGEOAnalyzer
from auto_fixes import AutoFixGenerator
from advanced_reporting import AdvancedReportGenerator

# .env f√°jl bet√∂lt√©se
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

class GEOAnalyzer:
    """Generative Engine Optimization elemz≈ë oszt√°ly"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or GOOGLE_API_KEY
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; GEO-Analyzer/1.0)'
        })
    
    def validate_url(self, url: str) -> bool:
        """URL valid√°l√°s"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False
    
    def check_robots_txt(self, url: str) -> Tuple[bool, str]:
        """Robots.txt ellen≈ërz√©s fejlettebb hibakezel√©ssel"""
        parsed_url = urlparse(url)
        robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
        
        try:
            rp = RobotFileParser()
            rp.set_url(robots_url)
            rp.read()
            can_fetch = rp.can_fetch("*", url)
            return can_fetch, robots_url
        except Exception as e:
            print(f"    ‚ö†Ô∏è Robots.txt ellen≈ërz√©si hiba: {e}")
            return True, robots_url  # Ha nincs robots.txt, enged√©lyezz√ºk
    
    def check_sitemap(self, url: str) -> Tuple[bool, str, Optional[int]]:
        """Sitemap ellen≈ërz√©s t√∂bb helyen"""
        parsed_url = urlparse(url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        
        # Pr√≥b√°ljuk meg t√∂bb helyen is
        sitemap_locations = [
            f"{base_url}/sitemap.xml",
            f"{base_url}/sitemap_index.xml",
            f"{base_url}/sitemap.xml.gz",
            f"{base_url}/wp-sitemap.xml"  # WordPress
        ]
        
        for sitemap_url in sitemap_locations:
            try:
                r = self.session.get(sitemap_url, timeout=10)
                if r.status_code == 200:
                    # Ellen≈ërizz√ºk, hogy t√©nyleg sitemap-e
                    if 'xml' in r.headers.get('content-type', '').lower() or '<urlset' in r.text[:500]:
                        return True, sitemap_url, len(r.text)
            except requests.RequestException:
                continue
        
        return False, sitemap_locations[0], None
    
    def get_html(self, url: str) -> Optional[str]:
        """HTML lek√©r√©se fejlettebb hibakezel√©ssel"""
        try:
            r = self.session.get(url, timeout=15)
            r.raise_for_status()
            return r.text
        except requests.RequestException as e:
            print(f"    ‚ùå HTML lek√©r√©si hiba: {e}")
            return None
    
    def check_schema(self, html: str) -> Dict[str, any]:
        """Schema.org ellen≈ërz√©s r√©szletesebb elemz√©ssel"""
        soup = BeautifulSoup(html, 'html.parser')
        schemas = soup.find_all("script", type="application/ld+json")
        
        schema_info = {
            "count": {"FAQPage": 0, "HowTo": 0, "Organization": 0, "Product": 0, 
                     "Article": 0, "LocalBusiness": 0, "Other": 0},
            "details": [],
            "has_breadcrumbs": False,
            "has_search_action": False
        }
        
        for script in schemas:
            try:
                data = json.loads(script.string)
                items = data if isinstance(data, list) else [data]
                
                for item in items:
                    schema_type = item.get("@type")
                    
                    # T√∂bbf√©le t√≠pus kezel√©se
                    if isinstance(schema_type, list):
                        schema_type = schema_type[0] if schema_type else "Unknown"
                    
                    # Kategoriz√°l√°s
                    if schema_type in schema_info["count"]:
                        schema_info["count"][schema_type] += 1
                    else:
                        schema_info["count"]["Other"] += 1
                    
                    # Speci√°lis s√©m√°k detekt√°l√°sa
                    if schema_type == "BreadcrumbList":
                        schema_info["has_breadcrumbs"] = True
                    elif schema_type == "WebSite" and "potentialAction" in item:
                        schema_info["has_search_action"] = True
                    
                    # Schema r√©szletek t√°rol√°sa
                    schema_info["details"].append({
                        "type": schema_type,
                        "has_image": "@image" in str(item),
                        "has_rating": "aggregateRating" in str(item)
                    })
                    
            except json.JSONDecodeError as e:
                print(f"    ‚ö†Ô∏è Schema JSON parse hiba: {e}")
                continue
        
        return schema_info
    
    def check_meta_and_headings(self, html: str) -> Dict:
        """Metaadatok √©s heading strukt√∫ra r√©szletes elemz√©se"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # Title elemz√©s
        title = soup.title.string.strip() if soup.title and soup.title.string else None
        title_length = len(title) if title else 0
        
        # Meta description
        description_tag = soup.find('meta', attrs={'name': 'description'})
        description = description_tag.get('content', '').strip() if description_tag else None
        description_length = len(description) if description else 0
        
        # Open Graph √©s Twitter Card
        og_title = soup.find('meta', property='og:title')
        og_description = soup.find('meta', property='og:description')
        og_image = soup.find('meta', property='og:image')
        twitter_card = soup.find('meta', attrs={'name': 'twitter:card'})
        
        # Heading strukt√∫ra
        headings = {f"h{i}": len(soup.find_all(f"h{i}")) for i in range(1, 7)}
        
        # H1 elemz√©s
        h1_tags = soup.find_all('h1')
        h1_texts = [h1.get_text().strip() for h1 in h1_tags]
        
        return {
            "title": title,
            "title_length": title_length,
            "title_optimal": 30 <= title_length <= 60,
            "description": description,
            "description_length": description_length,
            "description_optimal": 120 <= description_length <= 160,
            "headings": headings,
            "h1_count": len(h1_tags),
            "h1_texts": h1_texts[:3],  # Max 3 H1 sz√∂veg
            "has_og_tags": bool(og_title or og_description or og_image),
            "has_twitter_card": bool(twitter_card),
            "heading_hierarchy_valid": self._check_heading_hierarchy(headings)
        }
    
    def _check_heading_hierarchy(self, headings: Dict) -> bool:
        """Heading hierarchia ellen≈ërz√©se"""
        # H1 kell legyen
        if headings.get('h1', 0) == 0:
            return False
        
        # Ne legyen t√∫l sok H1
        if headings.get('h1', 0) > 1:
            return False
        
        # Hierarchia ellen≈ërz√©s: ha van H3, legyen H2 is
        for i in range(3, 7):
            if headings.get(f'h{i}', 0) > 0 and headings.get(f'h{i-1}', 0) == 0:
                return False
        
        return True
    
    def check_mobile_friendly(self, html: str) -> Dict:
        """Mobile-friendly r√©szletes ellen≈ërz√©s"""
        soup = BeautifulSoup(html, 'html.parser')
        
        result = {
            "has_viewport": False,
            "viewport_content": None,
            "responsive_images": False,
            "text_size_adjustable": False
        }
        
        # Viewport meta tag
        viewport = soup.find('meta', attrs={'name': 'viewport'})
        if viewport:
            content = viewport.get('content', '')
            result["has_viewport"] = True
            result["viewport_content"] = content
            result["responsive_images"] = 'width=device-width' in content
        
        # Responsive k√©pek ellen≈ërz√©se
        images = soup.find_all('img')
        responsive_img_count = sum(1 for img in images if img.get('srcset') or 'responsive' in img.get('class', []))
        result["responsive_images"] = responsive_img_count > 0 if images else True
        
        return result
    
    def get_pagespeed_insights_with_retry(self, url: str, strategy: str = 'mobile', max_retries: int = 3) -> Optional[Dict]:
        """PageSpeed Insights API h√≠v√°s retry logik√°val √©s jobb hibakezel√©ssel"""
        if not self.api_key:
            return None
        
        for attempt in range(max_retries):
            try:
                endpoint = 'https://www.googleapis.com/pagespeedonline/v5/runPagespeed'
                
                params = {
                    'url': url,
                    'strategy': strategy,
                    'key': self.api_key,
                    'category': ['performance', 'seo']  # Csak a legfontosabbak a gyorsas√°g √©rdek√©ben
                }
                
                # Progressz√≠van n√∂vekv≈ë timeout
                timeout = 45 + (attempt * 15)  # 45, 60, 75 m√°sodperc
                
                print(f"    PageSpeed {strategy} pr√≥b√°lkoz√°s {attempt + 1}/{max_retries} (timeout: {timeout}s)")
                
                response = requests.get(endpoint, params=params, timeout=timeout)
                
                if response.status_code == 429:
                    wait_time = 60 + (attempt * 30)  # 60, 90, 120 m√°sodperc
                    print(f"    Rate limit - v√°rakoz√°s {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                elif response.status_code != 200:
                    print(f"    PageSpeed API hiba ({strategy}): {response.status_code}")
                    if attempt < max_retries - 1:
                        time.sleep(10)
                        continue
                    return None
                
                data = response.json()
                categories = data.get('lighthouseResult', {}).get('categories', {})
                
                psi = {}
                for cat in ['performance', 'seo']:
                    score = categories.get(cat, {}).get('score')
                    psi[cat] = round(score * 100) if score is not None else None
                
                # Core Web Vitals (ha van)
                audits = data.get('lighthouseResult', {}).get('audits', {})
                if audits:
                    psi['core_web_vitals'] = {
                        'lcp': audits.get('largest-contentful-paint', {}).get('displayValue'),
                        'fid': audits.get('max-potential-fid', {}).get('displayValue'),
                        'cls': audits.get('cumulative-layout-shift', {}).get('displayValue')
                    }
                
                print(f"    ‚úì PageSpeed sikeres ({strategy}): Perf {psi.get('performance', 'N/A')}, SEO {psi.get('seo', 'N/A')}")
                return psi
                
            except requests.exceptions.Timeout:
                print(f"    ‚è∞ Timeout ({strategy}) - {attempt + 1}. pr√≥b√°lkoz√°s")
                if attempt < max_retries - 1:
                    wait_time = 30 + (attempt * 15)
                    print(f"    V√°rakoz√°s {wait_time}s...")
                    time.sleep(wait_time)
                continue
            except requests.RequestException as e:
                print(f"    ‚ùå PageSpeed kapcsolati hiba ({strategy}): {str(e)[:100]}...")
                if attempt < max_retries - 1:
                    time.sleep(20)
                    continue
                return None
            except json.JSONDecodeError as e:
                print(f"    ‚ùå PageSpeed JSON parse hiba ({strategy}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(10)
                    continue
                return None
        
        print(f"    ‚ùå PageSpeed sikertelen {max_retries} pr√≥b√°lkoz√°s ut√°n ({strategy})")
        return None
    
    def calculate_ai_readiness_score(self, result: Dict) -> int:
        """AI-readiness score sz√°m√≠t√°s r√©szletesebb metrik√°kkal"""
        score = 0
        
        # Robots.txt (10 pont)
        if result.get('robots_txt', {}).get('can_fetch'):
            score += 10
        
        # Title √©s meta (20 pont)
        meta = result.get('meta_and_headings', {})
        if meta.get('title_optimal'):
            score += 10
        elif meta.get('title'):
            score += 5
        
        if meta.get('description_optimal'):
            score += 10
        elif meta.get('description'):
            score += 5
        
        # Heading strukt√∫ra (15 pont)
        if meta.get('heading_hierarchy_valid'):
            score += 10
        if meta.get('h1_count') == 1:
            score += 5
        
        # Schema.org (20 pont)
        schema = result.get('schema', {})
        schema_count = sum(schema.get('count', {}).values())
        if schema_count > 0:
            score += min(20, schema_count * 5)
        if schema.get('has_breadcrumbs'):
            score += 5
        
        # Sitemap (10 pont)
        if result.get('sitemap', {}).get('exists'):
            score += 10
        
        # Mobile-friendly (10 pont)
        mobile = result.get('mobile_friendly', {})
        if mobile.get('has_viewport'):
            score += 5
        if mobile.get('responsive_images'):
            score += 5
        
        # Social media (5 pont)
        if meta.get('has_og_tags'):
            score += 3
        if meta.get('has_twitter_card'):
            score += 2
        
        # PageSpeed (10 pont)
        psi = result.get('pagespeed_insights', {})
        if psi:
            mobile_psi = psi.get('mobile', {})
            if mobile_psi.get('seo'):
                score += min(10, mobile_psi['seo'] // 10)
        
        return min(100, score)  # Max 100 pont
    
    def analyze_url(self, url: str) -> Dict:
        """Egy URL teljes elemz√©se optimaliz√°lt PageSpeed h√≠v√°ssal"""
        if not self.validate_url(url):
            return {"url": url, "error": "√ârv√©nytelen URL"}
        
        result = {"url": url, "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")}
        
        print(f"\nüìä Elemz√©s: {url}")
        
        # Robots.txt
        print("  ü§ñ Robots.txt...")
        can_fetch, robots_url = self.check_robots_txt(url)
        result["robots_txt"] = {"url": robots_url, "can_fetch": can_fetch}
        
        # Sitemap
        print("  üó∫Ô∏è Sitemap...")
        sitemap_exists, sitemap_url, sitemap_size = self.check_sitemap(url)
        result["sitemap"] = {
            "exists": sitemap_exists, 
            "url": sitemap_url,
            "size_bytes": sitemap_size
        }
        
        # HTML lek√©r√©s
        print("  üìÑ HTML let√∂lt√©s...")
        html = self.get_html(url)
        if not html:
            result["error"] = "HTML nem el√©rhet≈ë"
            return result
        
        # HTML m√©ret
        result["html_size_kb"] = len(html) / 1024
        
        # Meta √©s headings
        print("  üìù Meta adatok...")
        result["meta_and_headings"] = self.check_meta_and_headings(html)
        
        # Schema
        print("  üèóÔ∏è Schema.org...")
        result["schema"] = self.check_schema(html)
        
        # Mobile-friendly
        print("  üì± Mobile teszt...")
        result["mobile_friendly"] = self.check_mobile_friendly(html)
        
        # AI-specifikus metrik√°k
        print("  üß† AI metrik√°k...")
        ai_metrics_analyzer = AISpecificMetrics()
        result["ai_metrics"] = ai_metrics_analyzer.analyze_ai_readiness(html, url)
        result["ai_metrics_summary"] = ai_metrics_analyzer.get_ai_readiness_summary(result["ai_metrics"])
        
        # Tartalom min≈ës√©g elemz√©s
        print("  üìä Tartalom elemz√©s...")
        content_analyzer = ContentQualityAnalyzer()
        content_quality = content_analyzer.analyze_content_quality(html, url)
        content_quality["overall_quality_score"] = content_analyzer.calculate_overall_quality_score(content_quality)
        result["content_quality"] = content_quality
        
        # Multi-platform GEO elemz√©s
        print("  üîó Platform kompatibilit√°s...")
        platform_analyzer = MultiPlatformGEOAnalyzer()
        result["platform_analysis"] = platform_analyzer.analyze_all_platforms(html, url)
        result["platform_suggestions"] = platform_analyzer.get_all_suggestions(result["platform_analysis"])
        result["platform_priorities"] = platform_analyzer.get_platform_priorities(result["platform_analysis"])
        
        # Index hint
        parsed = urlparse(url)
        result["index_hint"] = {
            "google_search_url": f"https://www.google.com/search?q=site:{parsed.netloc}",
            "bing_search_url": f"https://www.bing.com/search?q=site:{parsed.netloc}"
        }
        
        # PageSpeed Insights (csak ha van API kulcs)
        pagespeed_results = {}
        if self.api_key:
            print("  ‚ö° PageSpeed Insights...")
            
            # El≈ësz√∂r mobile (fontosabb)
            mobile_psi = self.get_pagespeed_insights_with_retry(url, 'mobile')
            if mobile_psi:
                pagespeed_results["mobile"] = mobile_psi
                
                # Desktop csak akkor, ha mobile sikeres volt
                print("  üíª PageSpeed Desktop...")
                desktop_psi = self.get_pagespeed_insights_with_retry(url, 'desktop', max_retries=2)  # Kevesebb retry desktop-ra
                if desktop_psi:
                    pagespeed_results["desktop"] = desktop_psi
            else:
                print("  ‚ö†Ô∏è PageSpeed √°tugr√°sa - t√∫l sok hiba")
        
        if pagespeed_results:
            result["pagespeed_insights"] = pagespeed_results
        
        # AI-readiness score
        result["ai_readiness_score"] = self.calculate_ai_readiness_score(result)
        
        # Automatikus jav√≠t√°si javaslatok
        print("  üîß Jav√≠t√°si javaslatok...")
        auto_fix_generator = AutoFixGenerator()
        result["auto_fixes"] = auto_fix_generator.generate_all_fixes(result, url)
        
        print(f"  ‚úÖ K√©sz! AI Score: {result['ai_readiness_score']}/100")
        
        return result
    
    def analyze_urls_parallel(self, url_list: List[str], max_workers: int = 2) -> List[Dict]:  # Cs√∂kkentett worker sz√°m
        """T√∂bb URL p√°rhuzamos elemz√©se"""
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.analyze_url, url): url for url in url_list}
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    results.append(result)
                    print(f"‚úì Befejezve: {url}")
                except Exception as e:
                    print(f"‚úó Hiba {url} elemz√©sekor: {e}")
                    results.append({"url": url, "error": str(e)})
        
        return results


def analyze_urls(url_list: List[str], api_key: Optional[str] = None, 
                output_file: str = "ai_readiness_full_report.json",
                parallel: bool = True, skip_pagespeed: bool = False) -> None:
    """F≈ë elemz≈ë f√ºggv√©ny fejlesztett opci√≥kkal"""
    
    analyzer = GEOAnalyzer(api_key)
    
    # Ha nincs API kulcs, ne pr√≥b√°lkozzunk PageSpeed-del
    if not analyzer.api_key:
        skip_pagespeed = True
    
    print(f"{'='*50}")
    print(f"üöÄ GEO Analyzer - {len(url_list)} URL elemz√©se")
    print(f"API kulcs: {'‚úÖ Van' if analyzer.api_key else '‚ùå Nincs'}")
    print(f"PageSpeed: {'‚ùå √Åtugr√°s' if skip_pagespeed else '‚úÖ Enged√©lyezve'}")
    print(f"P√°rhuzamos: {'‚úÖ Igen' if parallel and len(url_list) > 1 else '‚ùå Nem'}")
    print(f"{'='*50}")
    
    start_time = time.time()
    
    if parallel and len(url_list) > 1:
        results = analyzer.analyze_urls_parallel(url_list)
    else:
        results = []
        for url in url_list:
            result = analyzer.analyze_url(url)
            results.append(result)
    
    # Eredm√©nyek ment√©se
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    elapsed_time = time.time() - start_time
    
    print(f"\n{'='*50}")
    print(f"‚úÖ Elemz√©s befejezve!")
    print(f"‚è±Ô∏è Id≈ëtartam: {elapsed_time:.1f} m√°sodperc")
    print(f"üíæ Jelent√©s: {output_file}")
    print(f"{'='*50}")
    
    # √ñsszefoglal√≥ statisztik√°k
    valid_results = [r for r in results if 'ai_readiness_score' in r]
    if valid_results:
        avg_score = sum(r['ai_readiness_score'] for r in valid_results) / len(valid_results)
        print(f"\nüìä √Åtlagos AI-readiness score: {avg_score:.1f}/100")
        
        # Top 3 √©s Bottom 3
        sorted_results = sorted(valid_results, key=lambda x: x['ai_readiness_score'], reverse=True)
        
        print("\nüèÜ Legjobb 3 oldal:")
        for r in sorted_results[:3]:
            print(f"  ‚Ä¢ {r['url']}: {r['ai_readiness_score']}/100")
        
        if len(sorted_results) > 3:
            print("\nüîß Fejlesztend≈ë oldalak:")
            for r in sorted_results[-3:]:
                if r['ai_readiness_score'] < 50:
                    print(f"  ‚Ä¢ {r['url']}: {r['ai_readiness_score']}/100")


# P√©lda futtat√°s
if __name__ == "__main__":
    urls_to_test = [
        "https://www.example.com",
        "https://www.wikipedia.org",
        "https://www.github.com"
    ]
    
    # API kulcs a k√∂rnyezeti v√°ltoz√≥b√≥l
    api_key = GOOGLE_API_KEY
    
    if not api_key:
        print("‚ö†Ô∏è Figyelem: Google API kulcs nincs be√°ll√≠tva!")
        print("√Åll√≠tsd be a .env f√°jlban: GOOGLE_API_KEY=your_api_key")
        print("PageSpeed Insights n√©lk√ºl fut az elemz√©s.\n")
    
    # Gyorsabb futtat√°s: skip_pagespeed=True ha nincs sz√ºks√©g PageSpeed-re
    analyze_urls(urls_to_test, api_key, parallel=True, skip_pagespeed=False)
    
    # HTML report gener√°l√°s
    try:
        from report import generate_html_report
        generate_html_report()
    except ImportError:
        print("‚ö†Ô∏è report.py nem tal√°lhat√≥ - HTML jelent√©s kihagyva")